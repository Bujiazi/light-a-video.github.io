
<!-- saved from url=(0035)https://chail.github.io/anyres-gan/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>DreamBooth</title>

    <link href="./DreamBooth_files/css" rel="stylesheet">
    <script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
    <style>
        body {
            font-family: 'Open-Sans', sans-serif;
            font-weight: 300;
            background-color: #fff;
        }

        .content {
            width: 1000px;
            padding: 25px 50px;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        .contentblock {
            width: 950px;
            margin: 0 auto;
            padding: 0;
            border-spacing: 25px 0;
        }

        .contentblock td {
            background-color: #fff;
            padding: 25px 50px;
            vertical-align: top;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        a,
        a:visited {
            color: #224b8d;
            font-weight: 300;
        }

        #authors {
            text-align: center;
            margin-bottom: 20px;
        }

        #conference {
            text-align: center;
            margin-bottom: 20px;
            font-style: italic;
        }

        #authors a {
            margin: 0 10px;
        }

        h1 {
            text-align: center;
            font-size: 35px;
            font-weight: 300;
        }

        h2 {
            font-size: 30px;
            font-weight: 300;
        }

        code {
            display: block;
            padding: 10px;
            margin: 10px 10px;
        }

        p {
            line-height: 25px;
            text-align: justify;
        }

        p code {
            display: inline;
            padding: 0;
            margin: 0;
        }

        #teasers {
            margin: 0 auto;
        }

        #teasers td {
            margin: 0 auto;
            text-align: center;
            padding: 5px;
        }

        #teasers img {
            width: 250px;
        }

        #results img {
            width: 133px;
        }

        #seeintodark {
            margin: 0 auto;
        }

        #sift {
            margin: 0 auto;
        }

        #sift img {
            width: 250px;
        }

        .downloadpaper {
            padding-left: 20px;
            float: right;
            text-align: center;
        }

        .downloadpaper a {
            font-weight: bold;
            text-align: center;
        }

        #demoframe {
            border: 0;
            padding: 0;
            margin: 0;
            width: 100%;
            height: 340px;
        }

        #feedbackform {
            border: 1px solid #ccc;
            margin: 0 auto;
            border-radius: 15px;
        }

        #eyeglass {
            height: 530px;
        }

        #eyeglass #wrapper {
            position: relative;
            height: auto;
            margin: 0 auto;
            float: left;
            width: 800px;
        }

        #mitnews {
            font-weight: normal;
            margin-top: 20px;
            font-size: 14px;
            width: 220px;
        }

        #mitnews a {
            font-weight: normal;
        }

        .teaser-img {
            width: 80%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .teaser-gif {
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .summary-img {
            width: 100%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }

        .video-iframe {
            width: 1000;
            height: 800;
						margin:auto;
						display:block;
        }

      .container {
        display: flex;
        align-items: center;
        justify-content: center
      }
      .image {
        flex-basis: 40%
      }
      .text {
        font-size: 20px;
        padding-left: 20px;
      }
			.center {
				margin-left: auto;
				margin-right: auto;
			}
			.boxshadow {
				border: 1px solid;
				padding: 10px;
				box-shadow: 2px 2px 5px #888888;
			}
			.spacertr{
				height: 8px;
			}
			.spacertd{
				width: 40px;
			}

    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1 style="font-family:Copperplate;">DreamBooth: Fine Tuning Text-to-image Dissusion Models for Subject-Driven Generation</h1>
        <p id="authors">
            <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>
            <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>
            <a href="https://varunjampani.github.io/">Varun Jampani</a>
            <a href="https://research.google/people/106214/">Yael Pritch</a>
            <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>
			<a href="https://kfiraberman.github.io/">Kfir Aberman</a>
            <br>
            <!-- <strong>MIT Computer Science and Artificial Intelligence Laboratory</strong> -->
            Google Research
<!--            <br><i>European Conference on Computer Vision 2022</i>-->
        </p>
<!-- 				<font size="+2">
					<p style="text-align: center;">
						<a href="https://arxiv.org/abs/2204.07156" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://chail.github.io/anyres-gan/bibtex.txt" target="_blank">[Bibtex]</a>
					</p>
				</font>
				<font size="+1">
					<p style="text-align: center;">
						Skip to:  &nbsp;&nbsp;
						<a href="https://chail.github.io/anyres-gan/#abstract">[Abstract]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://chail.github.io/anyres-gan/#video">[Supplementary Video]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://chail.github.io/anyres-gan/#samples">[Random Samples]</a> &nbsp;&nbsp;&nbsp;&nbsp;
					</p>
				</font> -->
        <p>
            <img class="teaser-img" src="./DreamBooth_files/teaser_light.gif">
        </p>
        <br>

				<p id="abstract"><strong>Abstract: </strong>
Large text-to-image models achieved a remarkable leap in the evolution of AI, by enabling high-quality and diverse synthesis of images from a given text prompt. However, these models missing tools that allow users telling their own story, based on specific subjects as they lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. 
In this work, we present a new approach for personalization of text-to-image diffusion models. Given a few images of a subject, we fine tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. 
Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes. 
By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables us to synthesize the subject in diverse contexts, poses, views and lighting conditions that do not appear in the reference images but are adapted from objects belong to the same class and already exist in the output domain of the model. 
We apply our technique to various applications including re-contextualization, original artistic rendition generation, appearance modification, accessorizing, and many other previously unassailable tasks. 
				</p>
        <br clear="all">
    </div>
    <div class="content" id="summary">

        <h2 style="text-align:center;">Approach</h2>

          <!--
				<div class="container">
					<div class="image">
						<img src='img/dataset_construction.jpg' width=250px></img>
					</div>
					<div class="text">
						<p>The typical preprocessing pipeline for unconditional image synthesize resizes all images to the same size, which discards available pixels. We propose a training procedure which can leverage these additional pixels from higher resolution images for image synthesis. </p>
					</div>
				</div>
          -->
        <p style="text-align: left;">Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. "dog"), and returns a fine-tuned/"personalized'' text-to-image model that encodes a unique identifier that refers to the subject. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts.
        </p>
        <img class="summary-img" src="./DreamBooth_files/high_level.png" style="width:100%;">
        <br>
        <hr>
        <p style="text-align: left;">Given ~3-5 images of a subject we fine tune a text-to-image diffusion in two steps: (a) fine tuning the low-resolution text-to-image model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., "A photo of a [T] dog‚Äù), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject's class by injecting the class name in the text prompt (e.g., "A photo of a dog‚Äù). (b) fine-tuning the super resolution components with pairs of low-resolution and high-resolution images taken from our input images set, which enables us to maintain high-fidelity to small details of the subject.</p>
        <img class="summary-img" src="./DreamBooth_files/system.png" style="width:100%;">
        <br>
        <hr>
       <!--  <p style="text-align: center;">Our training pipeline can handle images of different resolutions. We resize the FFHQ dataset to validate design decisions, and additionally collect images from Flickr to build our multi-size datasets.</p>
        <img class="summary-img" src="./DreamBooth_files/dataset.jpg">
        <br>
        <hr>
        <p style="text-align: center;">By setting the coordinate grid appropriately, our generator is capable of synthesizing additional details as the resolution increases. 
        </p>
        <img class="summary-img" src="./DreamBooth_files/qualitative.jpg">
        <br>
        <hr>
        <p style="text-align: center;">As we extrapolate on this coordinate grid, the generated textures tend to deteriorate first, dependent on the native training image sizes.
        </p>
        <img class="summary-img" src="./DreamBooth_files/extrapolation.jpg">
        <br>
        <hr>
        <p style="text-align: center;">We can modify our approach to synthesize on a cylindrical image plane, which naturally creates 360 degree panoramas.
				<br>Click <a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/pano010-2.mp4">here</a> to view in video form.</p>
				<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/pano010-2.mp4"><img class="summary-img" src="./DreamBooth_files/pano010-2.gif"></a>
				<br>
        <img class="summary-img" src="./DreamBooth_files/pano010.png">
			</div>
			<div class="content" id="samples">
        <h2 style="text-align:center;">Random Samples</h2>
				<p style="text-align: center;">Click on the panels below to view randomly generated patches from our model in comparison to super-resolution models. Our model is trained to synthesize images at continuous resolutions from random noise samples, and thus is not directly supervised with low-resolution and high-resolution pairs from real images.</p>
				<table style="text-align: center;" class="center">
					<tbody><tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/SR/birds/">
								<p style="text-align: center;">Birds</p>
								<img src="./DreamBooth_files/panel_birds.jpg" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/SR/church/">
								<p style="text-align: center;">Church</p>
								<img src="./DreamBooth_files/panel_church.jpg" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/SR/mountain/">
								<p style="text-align: center;">Mountain</p>
								<img src="./DreamBooth_files/panel_mountain.jpg" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/SR/ffhq6k/">
								<p style="text-align: center;">FFHQ6K</p>
								<img src="./DreamBooth_files/panel_ffhq.jpg" style="width:150px">
							</a>
							</div>
            </td>
					</tr>
				</tbody></table>
				<p style="text-align: center;">Click on the panels below to view randomly generated full image samples from our model at different sizes.</p>
				<table style="text-align: center;" class="center">
					<tbody><tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/full/birds/">
								<p style="text-align: center;">Birds</p>
								<img src="./DreamBooth_files/panel_birds_0.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/full/church/">
								<p style="text-align: center;">Church</p>
								<img src="./DreamBooth_files/panel_church_0.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/full/mountain/">
								<p style="text-align: center;">Mountain</p>
								<img src="./DreamBooth_files/panel_mountain_0.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/other_projects/anyres_gan/supplementary_html/full/ffhq6k/">
								<p style="text-align: center;">FFHQ6K</p>
								<img src="./DreamBooth_files/panel_ffhq_0.png" style="width:150px">
							</a>
							</div>
            </td>
					</tr>
				</tbody></table>
				<br>
        <hr> -->
    </div>      
    <div class="content" id="references">

        <h2>BibTex</h2>

        <code>
			@article{ruiz2022dreambooth,<br>
				&nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Dissusion Models for Subject-Driven Generation},<br>
				&nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
				&nbsp;&nbsp;booktitle={arXiv preprint arXiv:2204.07156},<br>
				&nbsp;&nbsp;year={2022}<br>
			 }
				</code>
    </div>      
    <div class="content" id="acknowledgements">
          <p><strong>Acknowledgements</strong>:
          We thank Rinon Gal, Adi Zicher, Tali Dekel, Bill Freeman, Dilip Krishnan and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. 
					 <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). -->
    </p></div>



</body></html>