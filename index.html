<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>AnimateDiff</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>

<style>
  p.serif{
    font-family:"Times New Roman", Times, serif;
  }
  p.sansserif{
    font-family: Arial, Helvetica, sans-serif;
  }
</style>
  
</head>

<body>
<div class="content">
  <h1><strong>AnimateDiff: Animate Personalized Image Diffusion Models via a Tuning-free Framework</strong></h1>
  <p id="authors" class="serif">
    <a href="mailto:guoyuwei@pjlab.org.cn">Yuwei Guo<sup>1,3</sup></a>
    <a href="https://ceyuan.me/">Ceyuan Yang<sup>1</sup></a>
    <a href="https://anyirao.com/">Anyi Rao<sup>4</sup></a>
    <a href="https://wyhsirius.github.io/">Yaohui Wang<sup>1</sup></a>
    <a href="http://mmlab.siat.ac.cn/yuqiao/index.html">Yu Qiao<sup>1</sup></a>
    <a href="http://dahua.site/">Dahua Lin<sup>1,2</sup></a>
    <a href="https://daibo.info/">Bo Dai<sup>1</sup></a>
    <br>
    <br>
    <a><sup>1</sup>Shanghai AI Laboratory</a>
    <a><sup>2</sup>The Chinese University of Hong Kong</a><br>
    <a><sup>3</sup>Nanjing University</a>
    <a><sup>4</sup>Stanford University</a>
  </p>

  <font size="+2">
    <p style="text-align: center;" class="serif">
      <a href="" target="_blank" style="font-weight: bold;">[Arxiv]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank" style="font-weight: bold;">[Github repo]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="" target="_blank" style="font-weight: bold;">[BibTeX]</a>
    </p><br>
  </font>

  <div class="row">
    <div class="col">
      <img src="figs/girl-blossom.gif" class="img-fluid example-img"><br>
      <!-- <p class="serif" style="text-align: center; margin: 5px;"><a href="">Model Link</a></p> -->
    </div>
    <div class="col"><img src="figs/castle.gif" class="img-fluid example-img"></div>
    <div class="col"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
  </div>

  <div class="row">
    <div class="col"><img src="figs/halo.gif" class="img-fluid example-img"></div>
    <div class="col"><img src="figs/squrriel.gif" class="img-fluid example-img"></div>
    <div class="col"><img src="figs/man-dark.gif" class="img-fluid example-img"></div>
  </div>

  <p style="text-align: left; font-size: 18px; margin: 10px;" class="serif">
    Generated with CivitAI models:
    <a href="">ToonYou</a>
    <a href="">Lyriel</a>
    <a href="">majicMIX Realistic</a>
    <a href="">RCNZ Cartoon 3d</a>
  </p>

</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Abstract</p>
  <p style="font-size: 1em" class="serif">Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models (specializing them to users' needs). Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model (Imagen, although our method is not limited to a specific model) such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views, and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, appearance modification, and artistic rendering (all while preserving the subject's key features).</p>
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Pipeline</p>
  <p style="font-size: 1em" class="serif">Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts.</p>
  <img class="summary-img" src="./DreamBooth_files/high_level.png" style="width:100%;"> <br>
  <p style="font-size: 1em" class="serif">Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts.</p>
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Gallery</p>
  <p style="font-size: 1em" class="serif">Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts.</p>

  <div class="row">
    <div class="col-gallery"><img src="figs/girl-blossom.gif" class="img-fluid example-img"><br></div>
    <div class="col-gallery"><img src="figs/castle.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
  </div>
  <p class="serif" style="text-align: left; margin: 5px; margin-bottom: 20px">Model: <a href="">Link</a></p>

  <div class="row">
    <div class="col-gallery"><img src="figs/girl-blossom.gif" class="img-fluid example-img"><br></div>
    <div class="col-gallery"><img src="figs/castle.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
  </div>
  <p class="serif" style="text-align: left; margin: 5px; margin-bottom: 20px">Model: <a href="">Link</a></p>

  <div class="row">
    <div class="col-gallery"><img src="figs/girl-blossom.gif" class="img-fluid example-img"><br></div>
    <div class="col-gallery"><img src="figs/castle.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
  </div>
  <p class="serif" style="text-align: left; margin: 5px; margin-bottom: 20px">Model: <a href="">Link</a></p>

  <div class="row">
    <div class="col-gallery"><img src="figs/girl-blossom.gif" class="img-fluid example-img"><br></div>
    <div class="col-gallery"><img src="figs/castle.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
    <div class="col-gallery"><img src="figs/girl-closeup.gif" class="img-fluid example-img"></div>
  </div>
  <p class="serif" style="text-align: left; margin: 5px; margin-bottom: 20px">Model: <a href="">Link</a></p>

</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">BibTex</p>
  <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div>

<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
  </p>
</div> -->
</body>
</html>
